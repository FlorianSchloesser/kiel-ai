---
description: Understanding the Transformer
---

# Week 7 - Transformers Part 1

## Course session

**Explanatory Session Part 1**

Self-attention and multihead attention



**Hugging Face Introduction**

Library and Walk-through of HuggingFace101



**Explanatory Session Part 2**

Transformer Encoder and Positional Encoding



## To-do

ðŸ˜Š

Go through this excellent site explaining Transformers:&#x20;

{% embed url="http://jalammar.github.io/illustrated-transformer/" %}

ðŸ˜ŠðŸ˜Š

{% embed url="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html" %}

ðŸ˜ŠðŸ˜ŠðŸ˜Š

Look closer at the Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and go through a [tutorial](https://pytorch.org/tutorials/beginner/transformer\_tutorial.html) on how to use it for next token prediction.
