# Week 2 - Token & Embeddings revisted

### This week you will...

* Gain a broad understanding of how language models have evolved, from early methods like n-grams to advanced transformer architectures.
* Understand the significance and limitations of word embeddings and recurrent neural networks, including LSTMs.

### Slides

{% file src="../../.gitbook/assets/30_10_23_Token_and_embeddings_revisited.pdf" %}

### Learning Resources

* [Stanford CS224N](https://www.youtube.com/watch?v=rmVRLeJRkl4\&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) NLP with Deep Learning
* [Stanford XCS224U](https://www.youtube.com/watch?v=K\_Dh0Sxujuc\&list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp): NLU

### Until next week you should...

* [ ] Watch the last ten minutes of this [Stanford lecture](https://www.youtube.com/watch?v=wzfWHP6SXxY\&t=4366s) on Attention
* [ ] Watch the first 25 Minutes of this [Stanford lecture](https://www.youtube.com/watch?v=gKD7jPAdbpE\&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ\&index=8) on Attention
* [ ] Watch all 4 Videos of the [Rasa Attention Series](https://www.youtube.com/watch?v=yGTUuEx3GkA\&t=2s)
* [ ] Complete the Notebook and play around with different embeddings
* [ ] Optional: Watch this [Stanford lecture](https://www.youtube.com/watch?v=ptuGllU5SQQ\&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ\&index=9) on Transformers
